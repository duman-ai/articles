---
import Footer from '../components/Footer.astro';
import Head from '../components/Head.astro';
import Author from '../components/Author.astro';
import DumanLogo from '../components/DumanLogo.astro';
---

<Head />

<header>
    <DumanLogo />
    <h1 class="text-4xl font-bold text-gray-900 text-left">پیکربندی و تنظیم مدل‌های زبانی</h1>
    <div class="post-meta text-left text-sm text-gray-500">
        <span>🗓️ تاریخ انتشار: ۵ می ۲۰۲۵</span>
        <Author />
    </div>
</header>

<main class="max-w-3xl mx-auto p-6">
    <div class="post">
        <p>هنگام استفاده از مدل‌های زبانی فهرستی از متغیرهای قابل تنظیم را مشاهده خواهید کرد. شناختن این متغیرها برای بهره‌برداری مناسب از قابلیت‌های مدل‌ زبانی اهمیت دارد. بسته به نوع نیاز خود باید متغیر مناسب را پیدا کرده و تنظیم کنید. در ادامه مهم‌ترین متغیرهای قابل تنظیم در استفاده از مدل‌های زبانی معرفی می‌شود. قابل ذکر است تمامی مباحث شرح داده شده در این مقاله مربوط به LLM Inference است و ربطی به آموزش (Training) مدل‌های زبانی ندارد.</p>

        <h3 dir="ltr" class="text-2xl font-semibold text-left mt-8 text-gray-800">1. Max Length:</h3>
        <p> این متغیر حداکثر تعداد توکن‌های خروجی را مشخص می‌کند و در مدیریت هزینه‌های سرویس‌ LLM اهمیت دارد. هر چقدر تعداد توکن‌های خروجی بیشتر باشد، هزینه آن نیز بیشتر می‌شود (به صورت کلی هزینه استفاده از سرویس‌های LLM تابعی از سایز توکن‌های ورودی و خروجی آن است). قابل ذکر است که سایز توکن‌های ورودی مدل زبانی محدودیت دارد. این محدودیت با عنوان Context Window برای هر مدل مشخص می‌شود.</p>

        <h3 dir="ltr" class="text-2xl font-semibold text-left mt-8 text-gray-800">2. Stop Sequences:</h3>
        <p>مجموعه‌ای از رشته‌ها که پایان تولید متن را برای مدل زبانی مشخص می‌کند. یعنی هنگامی که رشته مورد نظر تولید شد، آنجا انتهای خروجی مدل زبانی است.</p>

        <h3 dir="ltr" class="text-2xl font-semibold text-left mt-8 text-gray-800">3. Frequency Penalty/ Presence Penalty:</h3>
        <p>هر دو متغیر برای کنترل میزان تکرار یک توکن و ایجاد تنوع در توکن‌های تولید شده به کار می‌روند. Presence Penalty برای محدود کردن تولید توکنی که قبلاً استفاده شده به کار می‌رود (مستقل از تعداد دفعات آن) در حالی که با Frequency Penalty این محدودیت به تعداد دفعات استفاده از آن بستگی دارد.</p>

        <h3 dir="ltr" class="text-2xl font-semibold text-left mt-8 text-gray-800">4. Structured Output:</h3>
        <p>تولید پاسخ با یک ساختمان داده مشخص در یکپارچگی مدل‌های زبانی با سایر ابزارها اهمیت بسیار زیادی دارد. Json و XML دو ساختمان داده بسیار کاربردی در این یکپارچگی هستند. استراتژی‌های مختلفی برای این موضوع معرفی شده است. ساده‌ترین روش این است که در پرامپ ساختار شکل خروجی را درخواست کنیم. اگرچه با پیشرفت مدل‌های زبانی تولید درخواست مورد نیاز کاربر دقیق‌تر شده ولی الزاماً این روش منجر به دریافت پاسخ صحیح نخواهد شد. برای حل این مشکل بسیاری از مدل‌های زبانی رایج مدل‌های خود را fine tune کرده و قابلیت‌هایی نظیر Function Calling، Tool Calling و یا Json Mode اضافه کرده‌اند. برای آشنایی بیشتر خواندن <a href="https://python.langchain.com/docs/how_to/structured_output/" class="text-blue-600">این</a>  مقاله پیشنهاد می‌شود.</p>

        <h3 dir="rtl" class="text-2xl font-semibold text-left mt-8 text-gray-800">5. متغیرهای نمونه‌برداری (LLM Inference Sampling):</h3>
        <p>روش‌های Sampling در LLM Inference روش‌هایی هستند که میزان تصادفی بودن (randomness) تولید توکن را کنترل می‌کنند. این روش‌ها می‌توانند منجر به تولید جواب‌های تکراری و پیش‌بینی پذیر شده و یا خلاقیت مدل در تولید پاسخ را بیشتر کنند. اگر هنگام استفاده از مدل‌های زبانی دقت کرده باشید، توکن‌های خروجی یک به یک تولید می‌شود، به همین جهت به آن autoregressive گفته می‌شود. مکانیزم مدل‌های زبانی به این صورت است که به مجموعه‌ای از توکن‌ها از ۰ تا ۱۰۰ درصد احتمال تخصیص داده و از میان آن‌ها توکن بعدی را انتخاب می‌کند. در تسک‌هایی نظیر QA، استنتاج و ریاضی و خلاصه‌سازی از میزان تصادفی بودن کاسته می‌شود و در تسک‌هایی نظیر ایده‌پردازی و نوشتن شعر به میزان تصادفی بودن افزوده می‌شود. البته بسته به نیاز خود می‌توانید میزان تصادفی بودن را تنظیم و کنترل کنید. در ادامه مهم‌ترین روش‌های Sampling معرفی می‌‌شود. روش‌های Sampling محدود به این موارد نیست. همچنین شما می‌توانید ترکیبی از روش‌های Sampling را به کار ببرید.</p>

        <h4 dir="ltr" class="text-2xl font-semibold text-left mt-8 text-gray-800">5.1 Temperature Sampling:</h4>
        <p>هر چقدر مقدار Temperature به صفر نزدیک باشد، پاسخ مدل پیش‌بینی پذیرتر می‌شود. در این حالت، توکن بعدی از میان توکن‌های با احتمال بالا انتخاب می‌شود. اگر مقدار Temperature زیاد باشد (نزدیک ۱ و بیشتر) توکن بعد به توکن‌های با احتمال بالا محدود نشده و تنوع بیشتری برای انتخاب توکن اعمال می‌شود، این موضوع منجر به افزایش خلاقیت و تصادفی بودن بیشتر در پاسخ می‌شود. مقدار Temperature صفر یا عدد حقیقی مثبت است. مقادیر بیشتر از ۱ رایج نیست.</p>

        <h4 dir="ltr" class="text-2xl font-semibold text-left mt-8 text-gray-800">5.2 Top K Sampling:</h4>
        <p>در این روش توکن بعدی به K توکن با بیش‌ترین احتمال محدود می‌شود. بدیهی است هر چقدر مقدار K کم باشد پاسخ مدل منسجم‌تر شده و میزان تصادفی بودن آن کمتر می‌شود. از طرفی با زیاد شدن K میزان تصادفی بودن در پاسخ بیشتر می‌شود. مقدار K عدد طبیعی است (یک و بیشتر از یک).</p>

        <h4 dir="ltr" class="text-2xl font-semibold text-left mt-8 text-gray-800">5.3 Top P Sampling:</h4>
        <p>در این روش، توکن بعدی به مجموعه‌ای که جمع احتمال آن‌ها بیشتر از P باشد محدود می‌شود. به این ترتیب بر خلاف Top K Sampling تعداد توکن‌های قابل انتخاب در هر گام ثابت نبوده و می‌تواند تغییر کند. هر چقدر مقدار P کم باشد، توکن‌های قابل انتخاب کمتر شده و در نتیجه تصادفی بودن پاسخ کمتر می‌شود. و هر چقدر مقدار P زیاد باشد، تعداد توکن‌های قابل انتخاب بیشتر شده و در نتیجه پاسخ تصادفی‌تر می‌شود. مقدار P یک عدد مثبت حقیقی بین ۰ و ۱ است.</p>

        <h4 dir="ltr" class="text-2xl font-semibold text-left mt-8 text-gray-800">5.4 Min-P Sampling:</h4>
        <p>در این روش، توکن بعدی به توکن‌هایی با احتمال P و بیشتر محدود می‌شود. هر چقدر مقدار P کمتر باشد، تعداد توکن‌های قابل انتخاب بیشتر شده و میزان تصادفی‌ بودن پاسخ نیز بیشتر می‌شود. در عین حال هر چقدر مقدار P بیشتر شود، تعداد توکن‌های قابل انتخاب کمتر شده و از میزان تصادفی بودن پاسخ کاسته می‌شود. مقدار P یک عدد مثبت حقیقی بین ۰ و ۱ است.</p>

        <h4 dir="rtl" class="text-2xl font-semibold text-left mt-8 text-gray-800">مطالعه بیشتر در:</h4>
        <ul class="list-disc pl-5 mt-4 text-gray-600">
            <li><a href="https://www.promptingguide.ai/introduction/settings" target="_blank" class="text-blue-600">Prompting Guide - Settings</a></li>
            <li><a href="https://aws.amazon.com/blogs/security/context-window-overflow-breaking-the-barrier/" target="_blank" class="text-blue-600">Context Window Overflow - AWS</a></li>
            <li><a href="https://python.langchain.com/docs/how_to/structured_output/" target="_blank" class="text-blue-600">Structured Output - LangChain</a></li>
            <li><a href="https://www.vellum.ai/llm-parameters/llm-streaming" target="_blank" class="text-blue-600">LLM Streaming - Vellum.ai</a></li>
        </ul>
    </div>

</main>

<Footer />
